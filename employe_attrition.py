# -*- coding: utf-8 -*-
"""Employe_attrition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gtrnicL5KS2jsvr6kTZEqB3cyaHqkV0F

# Employee Attrition Prediction Using Machine Learning

## Background
As a junior data scientist with a foundation in economics and management, I am on an exciting journey to deepen my expertise in data science, with a particular focus on advanced analytics and machine learning. This project represents a significant step in my learning process, applying theoretical knowledge to real-world business challenges.

## Project Overview
Employee attrition is a critical issue that can substantially impact a company's financial health and operational efficiency. For HR professionals and business leaders, understanding the factors driving employee attrition is crucial for developing effective retention strategies. This project aims to leverage the power of machine learning to predict and analyze employee attrition.

***NOTE : THIS PROJECT BUILT IS ON RAZUKI(sgjustino)'S PROJECT ON KAGGLE***

## Objectives
1. Develop predictive models for employee attrition using various supervised machine learning algorithms.
2. Compare the performance of different algorithms in the context of HR analytics.
3. Identify key factors contributing to employee attrition.
4. Gain hands-on experience in applying machine learning techniques to real-world business problems.
5. Live Deployment

## Methodology
We will employ a range of supervised machine learning algorithms, including:
- Logistic Regression
- Random Forest
- Gradient Boosting Models, etc

These models will be trained and tested on a comprehensive dataset containing various employee-related features, with attrition as the target variable.

## Expected Outcomes
Through this analysis, we aim to:
1. Create improved predictive models for employee attrition.
2. Uncover insights into the most significant factors influencing employee turnover.
3. Provide actionable recommendations for HR strategies to mitigate attrition risks.
4. Demonstrate the practical application of machine learning in solving HR-related challenges.

This project not only serves as a valuable learning experience in advanced analytics and machine learning but also showcases the potential of data-driven decision-making in human resource management.

This is a fictional dataset created by IBM data scientists to study attrition found in the link below: https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset/data

IBM. IBM HR Analytics Employee Attrition & Performance (Uploaded by Pavan Subhash) . Kaggle. Retrieved [18 Oct 2023] from [https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset/data].


The dataset was further analyzed and improved model by Razuki: [https://www.kaggle.com/code/sgjustino/ibm-hr-data-supervised-ml-algorithms]

***NOTE : THIS PROJECT BUILT IS ON RAZUKI(sgjustino)'S PROJECT ON KAGGLE***
"""

#from google.colab import drive

import numpy as np
import pandas as pd
from tabulate import tabulate

from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor

from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, make_scorer,precision_score, recall_score, f1_score, roc_auc_score, roc_curve
from sklearn.ensemble import RandomForestClassifier


#drive.mount('/content/drive')

file_path = pd.read_csv('github.com/chrisagunwa/employee_attrition/HR-Employee-ttrition.csv')
data = file_path

data

data.info()

pd.options.display.max_columns = 200

data.head()

data.tail()

data.describe()

data.shape

data.dtypes.value_counts()

"""***A quick look into our dataset shows:***
- We have our data in .csv file
- We have 35 total columns/features and 1470 rows
- Out of the 35 columns/features,  26 are numerical while 26 are categorical including ['attrition'] which is our target

Cleaning of data
"""

#checking for missing values to be sure
missing_value = data.isnull().sum()
print('Missing Values:',missing_value, sep='\n')

print('===============================================')

data_unique = data.nunique()
print('Unique data:', data_unique, sep='\n')

not_needed_columns = ['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours']
data.drop(columns=not_needed_columns, inplace=True)

attrition_counts = data['Attrition'].value_counts()
print(attrition_counts)

# Calculate percentages
total = len(data)
ax = sns.countplot(x='Attrition', data=data, palette=['red', 'green'], hue='Attrition', legend=False)

# Annotate each bar with the percentage
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height() / total)
    x = p.get_x() + p.get_width() / 2 - 0.05
    y = p.get_height()
    ax.annotate(percentage, (x, y), ha='center', va='bottom')

# Add titles and labels
plt.title('Attrition Distribution')
plt.xlabel('Attrition')
plt.ylabel('Count')

# Adjust figure size
plt.figure(figsize=(8, 6))

plt.show()

"""Another quick look shows that we have imbalanced data.  we have 1233 'No' and 237 'Yes'. This imbalance should be addressed prior to model training to prevent overemphasis towards 'No' class when our study focus is on the employees who attrite - 'Yes'.

For better understanding, the percentage of attrition is 83.9 for 'NO' and 16.1 percent for 'YES'

***The analysis commenced by exploring factors that have been extensively researched and shown to be associated with attrition, including Age, Income, Job Role, Years at Company, Job Satisfaction, and Work-Life Balance. Their distribution across the two Attrition categories will be examined. Several relevant research papers are provided for reviewers who may be interested in further reading. ***

Azeem, S. M., & Akhtar, N. (2014). The influence of work life balance and job satisfaction on organizational commitment of healthcare employees. International journal of human resource studies, 4(2), 18.




Ng, T. W., & Feldman, D. C. (2010). The relationships of age with job attitudes: A meta‐analysis. Personnel psychology, 63(3), 677-718.




Steel, R. P., & Ovalle, N. K. (1984). A review and meta-analysis of research on the relationship between behavioral intentions and employee turnover. Journal of applied psychology, 69(4), 673.


Wright, B. E., & Christensen, R. K. (2010). Public service motivation: A test of the job attraction–selection–attrition model. International Public Management Journal, 13(2), 155-176.
"""

# Key features creation
key_features = ['Age', 'MonthlyIncome', 'JobRole', 'YearsAtCompany', 'JobSatisfaction', 'WorkLifeBalance']

attrition_yes = data[data['Attrition'] == 'Yes']
attrition_no = data[data['Attrition'] == 'No']

# Examining distribution
fig, axes = plt.subplots(3, 2, figsize=(15, 10))

for ax, feature in zip(axes.flatten(), key_features):
    sns.histplot(data=attrition_yes, x=feature, ax=ax, color='red', label='Attrition: Yes', bins=30, kde=True, alpha=0.7)
    sns.histplot(data=attrition_no, x=feature, ax=ax, color='green', label='Attrition: No', bins=30, kde=True, alpha=0.7)

    ax.set_xlabel(feature, fontsize=14)
    ax.set_ylabel('Count', fontsize=14)
    ax.set_title(f'Distribution by {feature}', fontsize=14)
    ax.legend(fontsize=12)

# Adjust layout
plt.tight_layout()
plt.subplots_adjust(top=0.9)  # Adjust space for suptitle
fig.suptitle('Distribution of Key Features by Attrition', fontsize=18, fontweight='bold')  # No LaTeX, using bold for emphasis

plt.show()

# Checking job describtion by role

# Grouping by 'JobRole' and 'Attrition' to get the counts
job_role_counts = data.groupby(['JobRole', 'Attrition']).size().reset_index(name='Count')

# Iterate and print with a line space between each job role
current_role = None
for _, row in job_role_counts.iterrows():
    job_role = row['JobRole']
    if job_role != current_role:
        if current_role is not None:
            print()  #space
        current_role = job_role
    print(f"{row['JobRole']:30} {row['Attrition']:10} {row['Count']}")


plt.figure(figsize=(15,7))
sns.countplot(data=data, y='JobRole', hue='Attrition', palette=['red', 'green'])
plt.title('Job Role Distribution counts', fontsize=18, fontweight='bold')
plt.xlabel('Count', fontsize=13)
plt.ylabel('Job Role', fontsize=13)
plt.show()

"""The initial distribution suggests alignment with research-backed evidence, showing noticeable differences between employees who left and those who stayed, particularly in features like Age, Monthly Income, and Years at Company. However, class imbalance makes it difficult to visualize differences in Job Satisfaction and Work-Life Balance, despite these factors being linked to negative engagement, absenteeism, and attrition in organizational studies.

Certain roles, such as Sales Executive, Research Scientist, Laboratory Technician, and Sales Representative, experience higher attrition rates, while management roles like Director and Manager see lower attrition. Although Job Level might offer better visual representation, Job Role is expected to provide stronger qualitative insights.

Next, categorical variables will be encoded, and a correlation matrix will be constructed to examine relationships and detect potential multicollinearity issues.
"""

# Features and target
X = data.drop(columns=['Attrition'])
y = data['Attrition']

# Encoding categorical variables
categorical_cols = X.select_dtypes(include=[object]).columns.tolist()

X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

X = X.astype(int)

#Encode Attrition
y = LabelEncoder().fit_transform(y)

correlation_matrix = X.corr()
correlation_df = pd.DataFrame(correlation_matrix)

# Display the DataFrame as a table
print(tabulate(correlation_df, headers='keys', tablefmt='pretty', floatfmt=".2f"))

correlation_matrix = X.corr()
plt.figure(figsize=(20, 15))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', cbar=True, square=True, cbar_kws={'shrink': .75}, linewidths=0.5)
plt.title('Correlation Matrix of feature\n', fontweight='bold')
plt.show()

"""The predictors 'MonthlyIncome' and 'JobLevel' show a high correlation (0.95), suggesting overlapping information. Similarly, 'TotalWorkingYears' is strongly correlated with both 'JobLevel' (0.78) and 'MonthlyIncome' (0.77), which is expected since more experienced employees typically hold higher job levels and earn more. Additionally, 'YearsWithCurrManager', 'YearsAtCompany', and 'YearsInCurrentRole' are positively correlated, reflecting that employees tend to stay with the same managers and in the same roles over time.

In this analysis, predictors like 'MonthlyIncome', 'TotalWorkingYears', 'YearsInCurrentRole', and 'YearsWithCurrManager' will be removed based on a 0.7 correlation cutoff, retaining 'JobLevel' and 'YearsAtCompany' to reduce multicollinearity.

'MonthlyIncome' is highly correlated with job level and is sensitive, making it redundant and difficult for policy-related work. 'TotalWorkingYears' is captured by other variables like 'YearsAtCompany'. 'YearsInCurrentRole' and 'YearsWithCurrManager' offer limited additional insights beyond 'YearsAtCompany', especially considering organizational structures and functional specializations.
"""

X = X.drop(columns=['MonthlyIncome', 'TotalWorkingYears', 'YearsInCurrentRole', 'YearsWithCurrManager'])

"""# **Building model**"""

X.info()

"""**Logistic Regression**

We will first try to build a logistic regression model, which is specifically designed for binary classification problems and the most straightforward model in our case. Prior to model building, we will split the data into training and test dataset and balance the data using SMOTE (Synthetic Minority Over-sampling Technique) due to the highly imbalanced class distribution in Attrition.

In balancing, there are a couple of techniques commonly employed - weighting, down-sampling, up-sampling and SMOTE. Here, we will utilise SMOTE to combine both down-sampling of the majority class (Attrition - 'No') and up-sampling of the minority class (Attrition - 'Yes').
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Balance data
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

#Logistic Regression model

logistic_model = LogisticRegression(solver='liblinear')
logistic_model.fit(X_train_resampled, y_train_resampled)

y_pred = logistic_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", classification_rep)

ypp = logistic_model.predict_proba(X_test)[:, 1]

fpr, tpr, th = roc_curve(y_test, ypp)
auc = roc_auc_score(y_test, ypp)

lw = 2
plt.plot(fpr, tpr, color='green', lw=lw, label='ROC curve (area = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='red', lw=lw, linestyle='--')
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()


print('AUC-ROC Score (Logistic Regression):', auc)


#

"""Using the balanced sample generated by SMOTE, the logistic regression model achieved an F1 score of 0.51 and an AUC of 0.81. Given the small number of employees who left the company, this indicates that the linear model was unexpectedly effective in predicting attrition.

When working with imbalanced datasets, it is worth noting that the F1 score can be a more informative metric than AUC, as it balances precision and recall while being less influenced by class imbalance. It is also crucial to assess the results in relation to the study's focus. In this case, the accuracy score is less informative due to the lower emphasis on the minority class (Attrition - 'Yes'), as the majority class yielded better results.

To enhance the model's performance, hyperparameter tuning will be conducted for the logistic regression model using GridSearchCV with 5-fold cross-validation on the SMOTE-balanced dataset.
"""

param_grid_lr = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
}

# GrridSearchCV
grid_search_lr = GridSearchCV(LogisticRegression(solver='liblinear', max_iter=1000), param_grid_lr, cv=5, n_jobs=-1)
grid_search_lr.fit(X_train_resampled, y_train_resampled)

# Best parameters
best_params_lr = grid_search_lr.best_params_
best_estimator_lr = grid_search_lr.best_estimator_
best_estimator_lr.fit(X_train_resampled, y_train_resampled)

y_pred_tuned = best_estimator_lr.predict(X_test)
accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
conf_matrix_tuned = confusion_matrix(y_test, y_pred_tuned)
classification_rep_tuned = classification_report(y_test, y_pred_tuned)


print('\nBest Parameters (Logistic Regression):', best_params_lr)
print("Accuracy (Tuned Logistic Regression):", accuracy_tuned)
print("Confusion Matrix (Tuned Logistic Regression):\n", conf_matrix_tuned)
print("Classification Report (Logistic Regression):\n", classification_rep_tuned)


#AUC-ROC
y_prob_lr = best_estimator_lr.predict_proba(X_test)[:, 1]
fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_prob_lr)
auc_lr = roc_auc_score(y_test, y_prob_lr)


plt.figure(figsize=(8, 6))
plt.plot(fpr_lr, tpr_lr, color='blue', lw=2, label=f'AUC = {auc_lr:.2f}')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

print('AUC-ROC Score (GridSeachCV):', auc_lr)

"""With hyperparameter tuning, there is improvement in the F1 score from 0.51 to 0.57 for Attrition['Yes']. There's decline hoever in the AUC from 0.811 to 0.802. Nonetheless, given the relatively small sample size and the absence of longitudinal employee data, these scores are reasonable for aiding the company in predicting attrition and implementing tailored interventions.

The analysis will next explore the data using a different model: Random Forest. Considering the numerous correlations among the features, the low F1 score in the logistic regression may be due to more non-linear and complex relationships between the features and attrition. Additionally, there may be more noisy data, as the hyperparameter tuning did not yield significant benefits for the linear regression. This prompts the use of the Random Forest model to better address the potential presence of complex relationships and noisy data.

The process will proceed directly to hyperparameter tuning via GridSearchCV (with 5-fold cross-validation) on the SMOTE-balanced dataset for building the Random Forest model.
"""

# Random Forest
# Hyperparameter tunning

param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

#GridSerachCV
grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, n_jobs=-1)
grid_search_rf.fit(X_train_resampled, y_train_resampled)

# Best parameters
best_params_rf = grid_search_rf.best_params_
best_estimator_rf = grid_search_rf.best_estimator_
best_estimator_rf.fit(X_train_resampled, y_train_resampled)

y_pred_tuned_rf = best_estimator_rf.predict(X_test)
accuracy_tuned_rf = accuracy_score(y_test, y_pred_tuned_rf)
conf_matrix_tuned_rf = confusion_matrix(y_test, y_pred_tuned_rf)
classification_rep_tuned_rf = classification_report(y_test, y_pred_tuned_rf)


print('\nBest Parameters (Random Forest):', best_params_rf)
print("Accuracy (Tuned Random Forest):", accuracy_tuned_rf)
print("Confusion Matrix (Tuned Random Forest):\n", conf_matrix_tuned_rf)
print("Classification Report (Random Forest):\n", classification_rep_tuned_rf)


# AUC-ROC
y_prob_rf = best_estimator_rf.predict_proba(X_test)[:, 1]
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_prob_rf)
auc_rf = roc_auc_score(y_test, y_prob_rf)


plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label=f'AUC = {auc_rf:.2f}')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

print('AUC-ROC Score (Random forest):', auc_rf)

"""Interestingly, the hyperparameter-tuned Random Forest model did not perform as well as the logistic regression, achieving an F1 score of 0.36 for Attrition['Yes'] and an AUC score of 0.77.

Although there was a hypothesis regarding more complex and non-linear relationships between the features and attrition, the relatively poor performance of the Random Forest model suggests that the underlying data may not display strong non-linear dependencies.

The next step will involve improving the prediction model using the Gradient Boosting method. Since Gradient Boosting builds trees sequentially, giving greater weight to misclassified samples, it is possible that this approach will yield higher predictive statistics for attrition.

The process will again proceed directly to hyperparameter tuning via GridSearchCV (with 5-fold cross-validation) on the SMOTE-balanced dataset for building the Gradient Boosting model.
"""

#Gradient boosting

#hyperparameter tuning


param_grid_gb = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
}

#GridSearchCV

grid_search_gb = GridSearchCV(estimator=GradientBoostingClassifier(random_state=42), param_grid=param_grid_gb, cv=5, n_jobs=-1)

grid_search_gb.fit(X_train_resampled, y_train_resampled)
best_params_gb = grid_search_gb.best_params_
best_estimator_gb = grid_search_gb.best_estimator_
best_estimator_gb.fit(X_train_resampled, y_train_resampled)

y_pred_tuned_gb = best_estimator_gb.predict(X_test)
accuracy_tuned_gb = accuracy_score(y_test, y_pred_tuned_gb)
conf_matrix_tuned_gb = confusion_matrix(y_test, y_pred_tuned_gb)
classification_rep_tuned_gb = classification_report(y_test, y_pred_tuned_gb)

print('\nBest Parameters (Gradient Boosting):', best_params_gb)
print("Accuracy (Tuned Gradient Boosting):", accuracy_tuned_gb)
print("Confusion Matrix (Tuned Gradient Boosting):\n", conf_matrix_tuned_gb)
print("Classification Report (Gradient Boosting):\n", classification_rep_tuned_gb)

# AUC-ROC
y_prob_gb = best_estimator_gb.predict_proba(X_test)[:, 1]
fpr_gb, tpr_gb, thresholds_gb = roc_curve(y_test, y_prob_gb)
auc_gb = roc_auc_score(y_test, y_prob_gb)

plt.figure(figsize=(8, 6))
plt.plot(fpr_gb, tpr_gb, color='blue', lw=2, label=f'AUC = {auc_gb:.2f}')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0, 1])
plt.ylim([0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()


print('AUC-ROC Score (Gradient boosting):', auc_gb)

"""The hyperparameter-tuned Gradient Boosting model did not perform as well as the logistic regression, achieving an F1 score of 0.42 for Attrition['Yes'] and an AUC score of 0.78. However, it outperformed the Random Forest model, likely due to the benefits of the sequential learning algorithm. Ensemble methods tend to excel in capturing non-linear relationships and interactions between features. The relatively poor performance of both the Random Forest and Gradient Boosting models in this context suggests that the underlying data may not exhibit strong non-linear dependencies.

Following the analysis of this binary classification problem through three different models with hyperparameter tuning, the results will be examined collectively in the next section.

# **Results & Analysis**
"""

#models

models = [best_estimator_lr, best_estimator_rf, best_estimator_gb]
model_names = ['Logistic Regression', 'Random Forest', 'Gradient Boosting']

plt.figure(figsize=(10, 6))
auc_scores = []
precision_scores = []
recall_scores = []
f1_scores = []

for model, name in zip(models, model_names):
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob, pos_label=1)
    auc = roc_auc_score(y_test, y_prob)
    auc_scores.append(auc)


    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {auc:.2f})')
    y_pred = model.predict(X_test)
    precision = precision_score(y_test, y_pred, pos_label=1)
    recall = recall_score(y_test, y_pred, pos_label=1)
    f1 = f1_score(y_test, y_pred, pos_label=1)

    precision_scores.append(precision)
    recall_scores.append(recall)
    f1_scores.append(f1)

    plt.plot([0, 1], [0, 1], 'k--', label='Baseline')

plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('AUC-ROC Curve')
plt.legend()
plt.show()

# Consolidated Results
metrics_df = pd.DataFrame({
    'Model': model_names,
    'AUC': auc_scores,
    'Precision': precision_scores,
    'Recall': recall_scores,
    'F1 Score': f1_scores
})

print("\nModel Evaluation Metrics:")
print(metrics_df)

"""Looking at the data collectively, it is evident that the logistic regression model emerged as the top performer, achieving an F1 score of approximately 0.57 for Attrition['Yes'] and an AUC of 0.80. This outcome suggests the likely existence of linear relationships between attrition and the features, indicating that attrition is less dependent on non-linear or more complex relationships with other features. Additionally, when dealing with imbalanced datasets, it is important to note that the F1 score can be a more informative metric than AUC. The F1 score balances precision and recall and is less affected by class imbalance. While the accuracy score is more commonly used, it is less useful in this context due to the lower emphasis on the minority class (Attrition - 'Yes') because of the overemphasis on better results in the majority class. Overall, considering the relatively small sample of Attrition['Yes'] data and the lack of longitudinal employee data, the results from the logistic regression model are satisfactory and decent for aiding the company in predicting attrition and implementing tailored interventions.

On the other hand, the Random Forest and Gradient Boosting algorithms may not have performed well due to the relatively small to moderate-sized dataset. For these two ensemble methods, better predictive statistics are likely to be observed with a larger dataset, allowing them to fully exploit their capabilities. In this context, their complexity in building the model may have led to overfitting issues with the limited data size.
"""

# Examining top 10 Features
coefficients_lr = best_estimator_lr.coef_[0]
importance_df_lr = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': coefficients_lr})
importance_df_lr = importance_df_lr.sort_values(by='Coefficient', ascending=False)
top_10_features_lr = importance_df_lr.head(10)


print(top_10_features_lr)

coefficients_lr = best_estimator_lr.coef_[0]
importance_df_lr = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': coefficients_lr})
importance_df_lr = importance_df_lr.sort_values(by='Coefficient', ascending=False)
top_10_features_lr = importance_df_lr.head(10)

plt.figure(figsize=(10, 6))
sns.barplot(x='Coefficient', y='Feature', data=top_10_features_lr, palette='viridis')
plt.xlim(0, top_10_features_lr['Coefficient'].max())  # Set the x-axis limit from 0 to the maximum coefficient
plt.xlabel('Coefficient')
plt.ylabel('Feature')
plt.title('Top 10 Important Features (Logistic Regression)')
plt.show()

"""# **Discussion and Conclussion**

As the logistic regression model emerged as the winner in our dataset context, we plotted the top 10 features from the model for the prediction on attrition. Below, we listed some explanation for each of the feature's possible causality in attributing attrition. We will also leave out HourlyRate and MonthlyRate due to their relatively small coefficient which reflects little to no influence vis-a-vis other features on Attrition in real-world settings.

OverTime_Yes (1.509276): Overtime work may lead to burnout and decreased job satisfaction, which can contribute to attrition.

PerformanceRating (0.417478): Surprisingly, better performance rating is associated with a higher likelihood of attrition. This, however, suggest that top employees may have attrited in light of better prospects for their careers elsewhere.

BusinessTravel_Travel_Frequently (0.194502): Frequent travel may disrupt work-life balance, possibly contributing to attrition.

NumCompaniesWorked (Coefficient: 0.140618): Employees who have worked in more companies may be more willing to seek outside opportunities in line with their working history.

YearsSinceLastPromotion (0.090782): Lack of career advancement may lead to employee dissatisfaction and contribute to attrition.

Gender_Male (0.071276): Male employees are slightly more likely to attrite than female employees. This has no working research explanation and would warrant further investigation in the company. However, taking a look across employee data might suggest that males may have a higher tendency to score higher on the other relevant features in affecting attrition.

MaritalStatus_Single (0.048594): The lack of family commitment or other financial ties might have influence single employees to look for other career opportunies elsewhere.

DistanceFromHome (Coefficient: 0.034816): Commuting stress or a desire for proximity to home may play a role in influencing attrition.

In this analysis, the results suggest that addressing work-life balance issues related to overtime and frequent business travel may help reduce attrition rates. Additionally, HR departments should pay attention to employees who have not received promotions for an extended period, as this group exhibits a higher likelihood of attrition. Higher performing employees are also likely to be talent-scouted by competition. A focus on recognition of work and tangible rewards should be emphasized in performance ranking systems, while employee engagement initiatives and personalised retention strategies may be useful through predicting potential employees who are thinking of leaving. Care has to be taken, however, in how HR policies are implemented to avoid the case of over-classification or targeting of employees on a basis on higher likelihood to leave the company from past data.

For the model building, it is interesting that the more simple logistic regression emerged as a winner as compared with more powerful and complex models. This also bring about the importance of starting with simple models and the linkage to Occam's razor principle. In this exercise, I had learnt more about the different modeling approaches and also see how the translation of model building intertwine with the necessity to understand the real-world business question through employee data and attrition. Looking at the study approach, it is possible that the selected features, while important individually, may not capture the full complexity of attrition patterns in the organization. Attrition often results from a combination of factors, including personal preferences, job satisfaction, and career advancement opportunities, which may not be fully represented in the available data. Patterns on attrition should also be viewed longitudinally, as the multi-year experiences of employees will have a larger influence on their decision to leave the company
"""

